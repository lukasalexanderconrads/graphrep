{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import igraph\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = igraph.Graph.Read_Pickle('ogbn-arxiv.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vertices: 169343\n",
      "number of edges: 1166243\n"
     ]
    }
   ],
   "source": [
    "print('number of vertices:', g.vcount())\n",
    "print('number of edges:', g.ecount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects of 5 most cited articles:\n",
      "16\n",
      "24\n",
      "16\n",
      "16\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# get list of in degree for each node (= number citations)\n",
    "indegree_list = g.indegree()\n",
    "\n",
    "# get ids of 5 most cited articles\n",
    "most_cited = np.argsort(indegree_list)[-5:]\n",
    "\n",
    "# print their subject areas\n",
    "print('subjects of 5 most cited articles:')\n",
    "for id in most_cited:\n",
    "    print(*g.vs['label'][id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject area that most articles referencing the 5 most cited articles belong to: 16\n"
     ]
    }
   ],
   "source": [
    "# get nodes with an edge to the most cited articles (= articles citing the most cited ones)\n",
    "citing_ids = []\n",
    "for id in most_cited:\n",
    "    citing_ids += g.predecessors(id)\n",
    "\n",
    "# remove any duplicates\n",
    "citing_ids = np.unique(citing_ids)\n",
    "\n",
    "# get the subject area of each article\n",
    "citing_subj = np.array(g.vs['label'])[citing_ids]\n",
    "\n",
    "# count occurence of each subject area\n",
    "subjects, counts = np.unique(citing_subj, return_counts=True)\n",
    "\n",
    "# print subject area with maximum occurrences\n",
    "print('subject area that most articles referencing the 5 most cited articles belong to:',\n",
    "      subjects[np.argmax(counts)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Split the Graph into Training and Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vertices by year\n",
    "vert_year = np.array(g.vs['year']).squeeze()\n",
    "# seperate vertices into two sets V_1, V_2\n",
    "vert_ids_1 = np.argwhere(vert_year < 2019)\n",
    "vert_ids_2 = np.argwhere(vert_year >= 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Vertex Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Triangle counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\n",
    "Let $G$ be a graph and $v \\in V(G)$.\n",
    "\n",
    "**Claim**: $\\Delta(v) = |E(G[\\mathcal{N}(v)])|$\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "For each pair\n",
    "$$u, w \\in \\mathcal{N}(v): \\{u, v\\}, \\{v, w\\} \\in E(G)$$\n",
    "by definition of a neighborhood.\n",
    "\n",
    "Therefore\n",
    "$${u, v, w}\\; \\text{form a triangle} \\;\\Leftrightarrow \\;{u, w} \\in E(G[\\mathcal{N}(v)]),$$\n",
    "\n",
    "so the number of triangles that include $v$ is equal to the number of edges in the neigborhood of $v$:\n",
    "\n",
    "$$\\Delta(v) = |E(G[\\mathcal{N}(v)])|.$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_triangles = []\n",
    "# for each vertex\n",
    "for id in range(g.vcount()):\n",
    "    # get its neighborhood (ignoring edge directions)\n",
    "    neighborhood = g.neighbors(id)\n",
    "    # create undirected subgraph induced by neigborhood\n",
    "    subg = g.induced_subgraph(neighborhood)\n",
    "    subg.to_undirected()\n",
    "    # append number of edges in subgraph to triangle counts\n",
    "    n_triangles.append(subg.ecount())\n",
    "\n",
    "# convert to numpy array\n",
    "n_triangles = np.array(n_triangles)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Other Vertex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing degree took 0.018128156661987305 seconds\n",
      "computing page rank took 0.3403968811035156 seconds\n",
      "computing coreness took 0.153914213180542 seconds\n",
      "computing eigenvector centrality took 1.0849132537841797 seconds\n",
      "computing indegree took 0.018941640853881836 seconds\n",
      "computing outdegree took 0.01842641830444336 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1. degree\n",
    "start = time()\n",
    "degree = g.degree(range(g.vcount()))\n",
    "print(f'computing degree took {time() - start} seconds')\n",
    "\n",
    "# 2. page rank score\n",
    "start = time()\n",
    "page_rank = g.pagerank() \n",
    "print(f'computing page rank took {time() - start} seconds')\n",
    "\n",
    "# 3. coreness\n",
    "start = time()\n",
    "coreness = g.coreness()\n",
    "print(f'computing coreness took {time() - start} seconds')\n",
    "\n",
    "# 4. eigenvector centrality\n",
    "start = time()\n",
    "ev_centrality = g.eigenvector_centrality()\n",
    "print(f'computing eigenvector centrality took {time() - start} seconds')\n",
    "\n",
    "# 5. indegree\n",
    "start = time()\n",
    "indegree = g.indegree(range(g.vcount()))\n",
    "print(f'computing indegree took {time() - start} seconds')\n",
    "\n",
    "# 6. outdegree\n",
    "start = time()\n",
    "outdegree = g.outdegree(range(g.vcount()))\n",
    "print(f'computing outdegree took {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all features into a matrix\n",
    "feature_mat = np.stack((degree, page_rank, coreness, ev_centrality, indegree, triangleness), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a decision tree as the classifier\n",
    "classifier = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data:\n",
    "# document representation as predictor (x), subject as response (y)\n",
    "x_train = np.array(g.vs['attr'])[vert_ids_1].squeeze()\n",
    "y_train = np.array(g.vs['label'])[vert_ids_1].squeeze()\n",
    "\n",
    "x_test = np.array(g.vs['attr'])[vert_ids_2].squeeze()\n",
    "y_test = np.array(g.vs['label'])[vert_ids_2].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tree to the train data\n",
    "classifier = classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2679258481986709\n"
     ]
    }
   ],
   "source": [
    "# test the tree on the test data and report accuracy\n",
    "accuracy = classifier.score(x_test, y_test)\n",
    "print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "# create induced subgraphs\n",
    "g_train = g.induced_subgraph(vert_ids_1.squeeze())\n",
    "g_test = g.induced_subgraph(vert_ids_2.squeeze())\n",
    "\n",
    "x = []\n",
    "for subg in [g_train, g_test]:\n",
    "    attr_array = np.array(subg.vs['attr'])\n",
    "    feature_list = []\n",
    "\n",
    "    for vert_id in range(subg.vcount()):\n",
    "        # get features of neighbors and self\n",
    "        neighbor_ids = subg.neighbors(vert_id) + [vert_id]\n",
    "        neighbor_feats = attr_array[neighbor_ids]\n",
    "        # compute their mean\n",
    "        feat_mean = np.mean(neighbor_feats, axis=0)\n",
    "        feature_list.append(feat_mean)\n",
    "    # combine mean features and raw features\n",
    "    x.append(np.concatenate((attr_array, feature_list), axis=1))\n",
    "\n",
    "# seperate into train and test splits\n",
    "x_train = x[0]\n",
    "x_test = x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier = classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.28625804991461434\n"
     ]
    }
   ],
   "source": [
    "# test and report accuracy\n",
    "accuracy = classifier.score(x_test, y_test)\n",
    "print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
